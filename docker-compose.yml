version: "3.8"

services:
  backend:
    build:
      context: .
      dockerfile: Dockerfile.backend
    ports:
      - "8000:8000"
    volumes:
      - ~/.finalyzer:/root/.finalyzer
      - ./backend:/app/backend
    environment:
      - LLM_PROVIDER=${LLM_PROVIDER:-ollama}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - OLLAMA_HOST=${OLLAMA_HOST:-http://host.docker.internal:11434}
      - DEV_MODE=${DEV_MODE:-true}
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: unless-stopped

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "3000:3000"
    environment:
      - NEXT_PUBLIC_API_URL=http://localhost:8000
    depends_on:
      - backend
    restart: unless-stopped

# Note: Ollama should be run separately on the host machine
# Install from https://ollama.ai and run:
#   ollama pull llama3.2
#   ollama pull nomic-embed-text

